{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a43717e8-8390-4dee-b92e-3e83780c2020",
    "_uuid": "a62f437a31c25637df14dd16aa1cc533f0cf3727"
   },
   "source": [
    "## 1. Introduction\n",
    "In this notebook I present an attempt to extend Keras DirectoryIterator to include reading .`wav` files. In the process the audio files are converted in spectrogram in log scale and returns a batch of single channel images. For demo purposes kernel will only handle 10 classes `'yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go'`. For the `silence` class we need to preprocess `__background_noise_` folder. For the `unknown` class we can create an additional folder containing samples from other audio files. \n",
    "\n",
    "In summary in this post:\n",
    "* We define a Keras compatible directory iterator that handle `.wav` files by converting them to numpy arrays (images)\n",
    "*  We use function `spect_loader` extracted from (https://github.com/adiyoss/GCommandsPytorch/blob/master/gcommand_loader.py) to calculate the spectrogram of the audio file\n",
    "* Image size is automatically calculated from the parameters of `spect_loader`. In this notebook we create single channel images with size `101x161` and `61x161`.\n",
    "* All images created are single channel. That rules out transfer learning (at the moment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "460273a5-301a-4ae3-8499-3abea02ffe0c",
    "_uuid": "24a608fd3a61c1b3e441427d35e688acb1b1ad7b"
   },
   "source": [
    "## 2. Imports  / Definitions\n",
    "Here we list all relevant imports and definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "084109bc-938d-48a1-9852-92e98b329f32",
    "_uuid": "1875e63c3a36b2b342f2871cf5e3a7f9f9efcb4e",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechtest\n",
      "tensorflow-speech-recognition-challenge\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import Sequential, Input\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import Iterator\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "import librosa\n",
    "import os\n",
    "import multiprocessing.pool\n",
    "from functools import partial\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "train_path = '../input/train/audio/'\n",
    "test_path = '../input/test.7z'\n",
    "\n",
    "#\n",
    "# The classes correspond to directory names under ../train/audio\n",
    "classnames = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cb9c4f91-0826-4fce-a8b5-d5fa78e99454",
    "_uuid": "4ab7ec081967e45155a952e576aa89a8a22ae15a"
   },
   "source": [
    "## 3. Directory Iterator definition\n",
    "In the cell below we define the `SpeechDirectoryIterator` which is subclass of  Keras `Iterator` and handles `.wav` files  by converting them to single channel images (numpy arrays) . In addition to the common Iterator arguments we added the following parameters that controlo the image (spectrogram) generation:\n",
    "* `window_size`: this quantity times the sampling rate (`sr`) returns the fft window size\n",
    "* `window_stride`: Each frame of audio is windowed by `window_stride * sr` \n",
    "* `window_type`: The type of window function to be applied (`hamming`, `hanning`, etc)\n",
    "* `normalize`: True / Fale \n",
    "* `max_len`: Keep only `max_len` frequency components (first dimension of the output numpy array)\n",
    "* `logit`: If true we get the `np.log1p` of the spectrogram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "75a679f4-ed82-454f-9909-e538572a881e",
    "_uuid": "42aeacbf39da37a673057e0c7be6baf395faa557",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spect_loader(path, window_size, window_stride, window, normalize, max_len=101, logit=True):\n",
    "    y, sr = librosa.load(path, sr=None)\n",
    "    # n_fft = 4096\n",
    "    n_fft = int(sr * window_size)\n",
    "    win_length = n_fft\n",
    "    hop_length = int(sr * window_stride)\n",
    "\n",
    "    # STFT\n",
    "    D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length,\n",
    "                     win_length=win_length, window=window)\n",
    "    spect, phase = librosa.magphase(D)\n",
    "\n",
    "    if logit==True:\n",
    "        #S = log(S+1)\n",
    "        spect = np.log1p(spect)\n",
    "    # make all spects with the same dims\n",
    "    # TODO: change that in the future\n",
    "    if spect.shape[1] < max_len:\n",
    "        pad = np.zeros((spect.shape[0], max_len - spect.shape[1]))\n",
    "        spect = np.hstack((spect, pad))\n",
    "    elif spect.shape[1] > max_len:\n",
    "        spect = spect[:, :max_len]\n",
    "    spect = np.resize(spect, (1, spect.shape[0], spect.shape[1]))\n",
    "    #spect = torch.FloatTensor(spect)\n",
    "\n",
    "    # z-score normalization\n",
    "    if normalize:\n",
    "        mean = np.mean(np.ravel(spect))\n",
    "        std = np.std(np.ravel(spect))\n",
    "        if std != 0:\n",
    "            spect = spect -mean\n",
    "            spect = spect / std\n",
    "\n",
    "    return spect\n",
    "\n",
    "def _count_valid_files_in_directory(directory, white_list_formats, follow_links):\n",
    "    \"\"\"Count files with extension in `white_list_formats` contained in a directory.\n",
    "    # Arguments\n",
    "        directory: absolute path to the directory containing files to be counted\n",
    "        white_list_formats: set of strings containing allowed extensions for\n",
    "            the files to be counted.\n",
    "    # Returns\n",
    "        the count of files with extension in `white_list_formats` contained in\n",
    "        the directory.\n",
    "    \"\"\"\n",
    "    def _recursive_list(subpath):\n",
    "        return sorted(os.walk(subpath, followlinks=follow_links), key=lambda tpl: tpl[0])\n",
    "\n",
    "    samples = 0\n",
    "    for root, _, files in _recursive_list(directory):\n",
    "        for fname in files:\n",
    "            is_valid = False\n",
    "            for extension in white_list_formats:\n",
    "                if fname.lower().endswith('.' + extension):\n",
    "                    is_valid = True\n",
    "                    break\n",
    "            if is_valid:\n",
    "                samples += 1\n",
    "    return samples\n",
    "\n",
    "def _list_valid_filenames_in_directory(directory, white_list_formats,\n",
    "                                       class_indices, follow_links):\n",
    "    \"\"\"List paths of files in `subdir` relative from `directory` whose extensions are in `white_list_formats`.\n",
    "    # Arguments\n",
    "        directory: absolute path to a directory containing the files to list.\n",
    "            The directory name is used as class label and must be a key of `class_indices`.\n",
    "        white_list_formats: set of strings containing allowed extensions for\n",
    "            the files to be counted.\n",
    "        class_indices: dictionary mapping a class name to its index.\n",
    "    # Returns\n",
    "        classes: a list of class indices\n",
    "        filenames: the path of valid files in `directory`, relative from\n",
    "            `directory`'s parent (e.g., if `directory` is \"dataset/class1\",\n",
    "            the filenames will be [\"class1/file1.jpg\", \"class1/file2.jpg\", ...]).\n",
    "    \"\"\"\n",
    "    def _recursive_list(subpath):\n",
    "        return sorted(os.walk(subpath, followlinks=follow_links), key=lambda tpl: tpl[0])\n",
    "\n",
    "    classes = []\n",
    "    filenames = []\n",
    "    subdir = os.path.basename(directory)\n",
    "    basedir = os.path.dirname(directory)\n",
    "    for root, _, files in _recursive_list(directory):\n",
    "        for fname in sorted(files):\n",
    "            is_valid = False\n",
    "            for extension in white_list_formats:\n",
    "                if fname.lower().endswith('.' + extension):\n",
    "                    is_valid = True\n",
    "                    break\n",
    "            if is_valid:\n",
    "                classes.append(class_indices[subdir])\n",
    "                # add filename relative to directory\n",
    "                absolute_path = os.path.join(root, fname)\n",
    "                filenames.append(os.path.relpath(absolute_path, basedir))\n",
    "    return classes, filenames\n",
    "\n",
    "class SpeechDirectoryIterator(Iterator):\n",
    "    \"\"\"Iterator capable of reading images from a directory on disk.\n",
    "    # Arguments\n",
    "       \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, directory, window_size, window_stride, \n",
    "                 window_type, normalize, max_len=101, logit=True,\n",
    "                 target_size=(256, 256), color_mode='grayscale',\n",
    "                 classes=None, class_mode='categorical',\n",
    "                 batch_size=32, shuffle=True, seed=None,\n",
    "                 data_format=None, save_to_dir=None,\n",
    "                 save_prefix='', save_format='png',\n",
    "                 follow_links=False, interpolation='nearest'):\n",
    "        if data_format is None:\n",
    "            data_format = K.image_data_format()\n",
    "        self.window_size = window_size\n",
    "        self.window_stride = window_stride\n",
    "        self.window_type = window_type\n",
    "        self.normalize = normalize\n",
    "        self.max_len = max_len\n",
    "        self.directory = directory\n",
    "        self.logit = logit\n",
    "#        self.image_data_generator = image_data_generator\n",
    "        self.target_size = tuple(target_size)\n",
    "        if color_mode not in {'rgb', 'grayscale'}:\n",
    "            raise ValueError('Invalid color mode:', color_mode,\n",
    "                             '; expected \"rgb\" or \"grayscale\".')\n",
    "        self.color_mode = color_mode\n",
    "        self.data_format = data_format\n",
    "        if self.color_mode == 'rgb':\n",
    "            if self.data_format == 'channels_last':\n",
    "                self.image_shape = self.target_size + (3,)\n",
    "            else:\n",
    "                self.image_shape = (3,) + self.target_size\n",
    "        else:\n",
    "            if self.data_format == 'channels_last':\n",
    "                self.image_shape = self.target_size + (1,)\n",
    "            else:\n",
    "                self.image_shape = (1,) + self.target_size\n",
    "        self.classes = classes\n",
    "        if class_mode not in {'categorical', 'binary', 'sparse',\n",
    "                              'input', None}:\n",
    "            raise ValueError('Invalid class_mode:', class_mode,\n",
    "                             '; expected one of \"categorical\", '\n",
    "                             '\"binary\", \"sparse\", \"input\"'\n",
    "                             ' or None.')\n",
    "        self.class_mode = class_mode\n",
    "        self.save_to_dir = save_to_dir\n",
    "        self.save_prefix = save_prefix\n",
    "        self.save_format = save_format\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "        white_list_formats = {'png', 'jpg', 'jpeg', 'bmp', 'ppm', 'wav'}\n",
    "\n",
    "        # first, count the number of samples and classes\n",
    "        self.samples = 0\n",
    "\n",
    "        if not classes:\n",
    "            classes = []\n",
    "            for subdir in sorted(os.listdir(directory)):\n",
    "                if os.path.isdir(os.path.join(directory, subdir)):\n",
    "                    classes.append(subdir)\n",
    "        self.num_classes = len(classes)\n",
    "        self.class_indices = dict(zip(classes, range(len(classes))))\n",
    "\n",
    "        pool = multiprocessing.pool.ThreadPool()\n",
    "        function_partial = partial(_count_valid_files_in_directory,\n",
    "                                   white_list_formats=white_list_formats,\n",
    "                                   follow_links=follow_links)\n",
    "        self.samples = sum(pool.map(function_partial,\n",
    "                                    (os.path.join(directory, subdir)\n",
    "                                     for subdir in classes)))\n",
    "\n",
    "        print('Found %d images belonging to %d classes.' % (self.samples, self.num_classes))\n",
    "\n",
    "        # second, build an index of the images in the different class subfolders\n",
    "        results = []\n",
    "\n",
    "        self.filenames = []\n",
    "        self.classes = np.zeros((self.samples,), dtype='int32')\n",
    "        i = 0\n",
    "        for dirpath in (os.path.join(directory, subdir) for subdir in classes):\n",
    "            results.append(pool.apply_async(_list_valid_filenames_in_directory,\n",
    "                                            (dirpath, white_list_formats,\n",
    "                                             self.class_indices, follow_links)))\n",
    "            \n",
    "        \n",
    "        for res in results:\n",
    "            classes, filenames = res.get()\n",
    "            self.classes[i:i + len(classes)] = classes\n",
    "            self.filenames += filenames\n",
    "            if i==0:\n",
    "                img = spect_loader(os.path.join(self.directory, filenames[0]), \n",
    "                               self.window_size, \n",
    "                               self.window_stride, \n",
    "                               self.window_type, \n",
    "                               self.normalize, \n",
    "                               self.max_len, \n",
    "                               self.logit) \n",
    "                img=np.swapaxes(img, 0, 2)\n",
    "                self.target_size = tuple((img.shape[0], img.shape[1]))\n",
    "                print(self.target_size)\n",
    "                if self.color_mode == 'rgb':\n",
    "                    if self.data_format == 'channels_last':\n",
    "                        self.image_shape = self.target_size + (3,)\n",
    "                    else:\n",
    "                        self.image_shape = (3,) + self.target_size\n",
    "                else:\n",
    "                    if self.data_format == 'channels_last':\n",
    "                        self.image_shape = self.target_size + (1,)\n",
    "                    else:\n",
    "                        self.image_shape = (1,) + self.target_size\n",
    "                        \n",
    "            i += len(classes)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        super(SpeechDirectoryIterator, self).__init__(self.samples, batch_size, shuffle, seed)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    def _get_batches_of_transformed_samples(self, index_array):\n",
    "        batch_x = np.zeros((len(index_array),) + self.image_shape, dtype=K.floatx())\n",
    "        batch_f = []\n",
    "        grayscale = self.color_mode == 'grayscale'\n",
    "        # build batch of image data\n",
    "        #print(index_array)\n",
    "        for i, j in enumerate(index_array):\n",
    "            #print(i, j, self.filenames[j])\n",
    "            fname = self.filenames[j]\n",
    "            #img = load_img(os.path.join(self.directory, fname),\n",
    "            #               grayscale=grayscale,\n",
    "            #               target_size=self.target_size,\n",
    "            #               interpolation=self.interpolation)\n",
    "            img = spect_loader(os.path.join(self.directory, fname), \n",
    "                               self.window_size, \n",
    "                               self.window_stride, \n",
    "                               self.window_type, \n",
    "                               self.normalize, \n",
    "                               self.max_len)\n",
    "            img=np.swapaxes(img, 0, 2)\n",
    "            \n",
    "            x = img_to_array(img, data_format=self.data_format)\n",
    "            #x = self.image_data_generator.random_transform(x)\n",
    "            #x = self.image_data_generator.standardize(x)\n",
    "            batch_x[i] = x\n",
    "            batch_f.append(fname)\n",
    "        # optionally save augmented images to disk for debugging purposes\n",
    "        if self.save_to_dir:\n",
    "            for i, j in enumerate(index_array):\n",
    "                img = array_to_img(batch_x[i], self.data_format, scale=True)\n",
    "                fname = '{prefix}_{index}_{hash}.{format}'.format(prefix=self.save_prefix,\n",
    "                                                                  index=j,\n",
    "                                                                  hash=np.random.randint(1e7),\n",
    "                                                                  format=self.save_format)\n",
    "                img.save(os.path.join(self.save_to_dir, fname))\n",
    "        # build batch of labels\n",
    "        if self.class_mode == 'input':\n",
    "            batch_y = batch_x.copy()\n",
    "        elif self.class_mode == 'sparse':\n",
    "            batch_y = self.classes[index_array]\n",
    "        elif self.class_mode == 'binary':\n",
    "            batch_y = self.classes[index_array].astype(K.floatx())\n",
    "        elif self.class_mode == 'categorical':\n",
    "            batch_y = np.zeros((len(batch_x), self.num_classes), dtype=K.floatx())\n",
    "            for i, label in enumerate(self.classes[index_array]):\n",
    "                batch_y[i, label] = 1.\n",
    "        else:\n",
    "            return batch_x\n",
    "        return batch_x, batch_y\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"For python 2.x.\n",
    "        # Returns\n",
    "            The next batch.\n",
    "        \"\"\"\n",
    "        with self.lock:\n",
    "            index_array = next(self.index_generator)[0]\n",
    "        # The transformation of images is not under thread lock\n",
    "        # so it can be done in parallel\n",
    "        return self._get_batches_of_transformed_samples(index_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "459f4259-ef07-441b-851f-6a7805d4f027",
    "_uuid": "5833373aef325d471f300b678d130d9a60738524",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 10 classes.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b441a8f45cff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m                                    \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassnames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                                    \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                                    seed=123)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-0163042271ef>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, window_size, window_stride, window_type, normalize, max_len, logit, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, interpolation)\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilenames\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m                 img = spect_loader(os.path.join(self.directory, filenames[0]), \n\u001b[0m\u001b[1;32m    191\u001b[0m                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_stride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "window_size=.02\n",
    "window_stride=.01\n",
    "window_type='hamming'\n",
    "normalize=True\n",
    "max_len=101\n",
    "batch_size = 64\n",
    "train_iterator = SpeechDirectoryIterator(directory=train_path, \n",
    "                                   batch_size=batch_size, \n",
    "                                   window_size=window_size, \n",
    "                                   window_stride=window_stride, \n",
    "                                   window_type=window_type,\n",
    "                                   normalize=normalize, \n",
    "                                   max_len=max_len, \n",
    "                                   classes=classnames, \n",
    "                                   shuffle=True, \n",
    "                                   seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c7560e5d-6e8b-407f-91e7-59f1d751d18e",
    "_uuid": "d559e523e9334747161ade3daaef350b5eb5078c"
   },
   "source": [
    "Let us take retrieve a batch from the iterator and display some \"sound\" spectrograms:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "61c56658-ec6c-4701-941d-c3c27da96116",
    "_uuid": "828717861012d6a8b496ca6afa8227f814cd2bd5",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_iterator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c81eab2eb97d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_figheight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_iterator' is not defined"
     ]
    }
   ],
   "source": [
    "train_iterator.reset()\n",
    "X, y = next(train_iterator)\n",
    "print(X.shape)\n",
    "f, axarr = plt.subplots(3, 3)\n",
    "f.set_figheight(8)\n",
    "f.set_figwidth(15)\n",
    "for i in range(9):\n",
    "    axarr[int(i/3), i%3].imshow(X[i, ..., 0], cmap='gray')\n",
    "    axarr[int(i/3), i%3].set_title(classnames[np.argmax(y[i])])\n",
    "plt.show()\n",
    "#plt.imshow(X[0, ..., 0], cmap='gray')\n",
    "#plt.title(classnames[np.argmax(y[0])])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3ab8a306-3265-4d7f-be5a-ec880baddfc3",
    "_uuid": "8ed8509d588c9e02ff876882ee4655e8ffe9bb1e"
   },
   "source": [
    "We can also define a different iterator with different parameters (eg. `max_len=61` no no logarithm scale). There is much space for experimentation here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "8160bc54-ec6f-488b-879b-14a95f2b6c0f",
    "_uuid": "dfcf2d9e484c9010881495197529c7706b978b87",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 10 classes.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-be875b711598>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                                    \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassnames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                    \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                                    seed=123)\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mtrain_iterator_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iterator_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-0163042271ef>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, window_size, window_stride, window_type, normalize, max_len, logit, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, interpolation)\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilenames\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m                 img = spect_loader(os.path.join(self.directory, filenames[0]), \n\u001b[0m\u001b[1;32m    191\u001b[0m                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_stride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "train_iterator_2 = SpeechDirectoryIterator(directory=train_path, \n",
    "                                   batch_size=batch_size, \n",
    "                                   window_size=window_size, \n",
    "                                   window_stride=window_stride, \n",
    "                                   window_type=window_type,\n",
    "                                   normalize=normalize, \n",
    "                                   logit = False, \n",
    "                                   max_len=61, \n",
    "                                   classes=classnames, \n",
    "                                   shuffle=True, \n",
    "                                   seed=123)\n",
    "train_iterator_2.reset()\n",
    "X, y = next(train_iterator_2)\n",
    "print(X.shape)\n",
    "f, axarr = plt.subplots(3, 3)\n",
    "f.set_figheight(8)\n",
    "f.set_figwidth(15)\n",
    "for i in range(9):\n",
    "    axarr[int(i/3), i%3].imshow(X[i, ..., 0], cmap='gray')\n",
    "    axarr[int(i/3), i%3].set_title(classnames[np.argmax(y[i])])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ac1bba93-ffa3-4762-94d7-f6e07c8340c3",
    "_uuid": "b40c09a531ae9517f2cf5b7860658ede28af74be"
   },
   "source": [
    "## 3. Train a simple CNN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "ae220cc4-5a69-4508-a62e-7b50171a5906",
    "_uuid": "e0ad8c2b7b71a9189b23fc21f13c0e194eaf915f",
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_iterator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d1f5a683cd6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_iterator' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(12, (5, 5), activation = 'relu', input_shape=train_iterator.image_shape))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(25, (5, 5), activation = 'relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(180, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(classnames), activation = 'softmax')) #Last layer with one output per class\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "# Make the model learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "20fca983-1745-429d-9664-7f74d77ee67c",
    "_uuid": "77b72079345e66f130ae6d02a80a55c98e776085",
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_iterator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-aca62bd26810>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.fit_generator(train_iterator,\n\u001b[0m\u001b[1;32m      2\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         verbose=1)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_iterator' is not defined"
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_iterator,\n",
    "        steps_per_epoch=np.ceil(train_iterator.n / batch_size),\n",
    "        epochs=3,\n",
    "        verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "df043030-7aa2-4699-8693-f1a86f0a4396",
    "_uuid": "f6a265b3381939d0d9a9027de523154e76ec5067",
    "collapsed": true
   },
   "source": [
    "## 4. Get predictions\n",
    "We are going to use the `predict_generator` method to extract predictions from the test set. First we need to define the correpsonding iterator. (Note: *At the moment I don't have access to the test dataset. I will update this section as soon as I can have access*)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "8caf3b02-d0bc-49cb-85d0-ca9842437284",
    "_kg_hide-output": true,
    "_uuid": "7632d8094657d7643fc44f005ceda51f9aeecc5b",
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/test.7z'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6cc08f651269>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                                    \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                    \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                                    shuffle=False)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-0163042271ef>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, window_size, window_stride, window_type, normalize, max_len, logit, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, interpolation)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/test.7z'"
     ]
    }
   ],
   "source": [
    "predict_iterator = SpeechDirectoryIterator(directory=test_path, \n",
    "                                   batch_size=batch_size, \n",
    "                                   window_size=window_size, \n",
    "                                   window_stride=window_stride, \n",
    "                                   window_type=window_type,\n",
    "                                   normalize=normalize, \n",
    "                                   max_len=max_len, \n",
    "                                   classes=None,\n",
    "                                   shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "65935762-b2f5-451b-9436-4a48b6faaf49",
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "_uuid": "f3195a6c6943d92aa6640e17ef7c16d23f73b667",
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict_iterator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-42789ee2a421>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m preds = model.predict_generator(generator=predict_iterator, \n\u001b[0m\u001b[1;32m      2\u001b[0m                         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                         verbose=1)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predict_iterator' is not defined"
     ]
    }
   ],
   "source": [
    "preds = model.predict_generator(generator=predict_iterator, \n",
    "                        steps=int(np.ceil(predict_iterator.n)/batch_size), \n",
    "                        verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "df394d03-f5f2-4ee2-8858-af4b1cfc6016",
    "_uuid": "87f1d8a3b3f7822ab9f15facdc0299e7e66dfa9a"
   },
   "source": [
    "## 5. Further considerations\n",
    "*  This kernel does not cover the train validation split issue. In my personal opinion we should first group by the subject id of the person who gave the voice command and then perform train/validation split. In this way we will avoid having the same actor saying the same word both in train and validation.\n",
    "* Single or multichannel images: It will be interestign to stack multiple information to create three channel image. Maybe theres will be some prospect with pretrained models\n",
    "* Image augmentation: This ranges from sound augmentation prior to spectrorgram creation, to standard image augmentation directly on the spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4c593b9d-b788-4ebe-9b64-ecd1178a6b6d",
    "_uuid": "68bf8d98e9d27d17468798396ad99033018a7cc8"
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
